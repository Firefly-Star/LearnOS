# 我们的OS还没有取名
本项目是一个在xv6-riscv的基础上进行重构优化和改进的，运行于Risc-V架构的类Unix操作系统。
截至初赛，我们的OS在xv6-riscv的基础上新增了19个系统调用，总计40个系统调用。
我们的OS对xv6-riscv作出的主要改进和提升体现在内存管理模块，进程通信模块，进程管理模块。
以下是我们各模块的完成情况。

这里一个表格
内存管理：(1)修改xv6的用户虚拟内存空间，新增虚拟内存段(2)实现伙伴系统和slab内存分配，实现O(1)的内核内存分配算法(3)实现懒分配和写时复制
进程通信：(1)实现信号量集机制(2)实现共享内存机制(3)实现消息队列
进程管理: (1)实现优先级队列调度(2)实现vfork

目录
一、概述
1.1 xxOS介绍
1.2 操作系统整体架构
1.3 目录和文件描述
二、xxOS的设计与实现
2.1 内存管理
2.1.1 概述
2.1.2 地址空间
2.1.3 内存映射
2.1.4 伙伴系统
2.1.5 slab分配器
2.1.6 懒分配
2.1.7 写时复制
2.2 进程通信
2.2.1 概述
2.2.2 进程控制块
2.2.3 IPC宏
2.2.3 共享内存
2.2.4 信号量
2.2.5 消息队列
2.3 进程管理
2.3.1 概述
2.3.2 优先级队列调度
2.3.3 vfork
2.4 辅助性的系统调用
2.4.1 时间相关
2.4.2 系统调用跟踪
三、总结与展望
3.1 工作总结
3.2 经验总结
3.3 未来计划


一、概述(TODO)
1.1 介绍
我们的操作系统基于 RISC-V 架构，运行于 RV64 指令集之上，并采用 SV39 虚拟内存管理机制，实际物理内存限制为 128MB。这种配置允许操作系统有效利用虚拟地址空间进行内存管理，同时满足特定的硬件资源约束与应用需求。
1.2 整体架构

1.3 目录和文件描述


二、xxOS的设计与实现
2.1 内存管理
2.1.1 概述
在 xxOS 操作系统中，内核态与用户态进程各自拥有独立的地址空间，这种内存隔离有效保护了内核数据结构，防止用户态程序的意外访问与篡改，从而降低了安全风险和系统不稳定性。

该机制依赖于内存管理单元（MMU），负责虚拟地址到物理地址的转换及不同地址空间的隔离。当用户态进程尝试访问受保护的内核资源时，MMU 会拦截此行为并生成页面错误（Page Fault），触发内核异常处理程序进行相应处理。

为了增强安全性，xxOS 使用 copyout 和 copyin 接口在内核态与用户态之间进行数据传输。这些接口要求显式指定数据源和目标内存区域，确保数据传输的完整性和一致性，并使错误处理更加明确和可追踪。

通过这种设计，xxOS 利用 MMU 的内存保护机制，确保仅授权的访问得以执行，从而进一步提升了系统的可靠性和安全性。

2.1.2 地址空间
在硬件层面的约定中，我们为 QEMU 虚拟机配置了 128MB 的物理内存。根据 RISC-V 架构规范，从 0x00000000 到 0x7FFFFFFF，被用于其他设备，而从 0x80000000(KERNBASE) 起的地址空间则预留用于引导程序和内核。因此，物理内存管理重点关注地址范围 KERNBASE 至 0x87FFFFFF(PHYSTOP) 之间的空间。

在采用 SV39 虚拟内存管理机制的背景下，理论上虚拟地址可扩展至 39 位。然而，考虑到 RISC-V 设计中对有符号扩展的支持，最高位用于符号表示可能会增加虚拟地址处理的复杂性。因此，我们将有效虚拟地址限制为 38 位，主要针对 0x0 至 0x3FFFFFFFFF(MAX_VA) 的范围进行内存管理。

结合这一约定，xxOS 对内核空间和用户空间作了以下划分：

内核空间布局
![内核空间布局](images/内核空间布局.png)

其中，在 KERNBASE 至 PHYSTOP 之间的空间的布局如下图所示。该布局由 kernel.ld 文件定义。
![我们管理的内存布局](images/我们管理的内存空间布局.jpg)

用户空间布局

用户空间的布局如下图所示，其中包括各个段的分配与映射。
![用户空间布局](images/用户空间布局.jpg)

对这两个图做解释

2.1.3 内存映射
xxOS 采用 RV64 指令集与 SV39 地址管理机制。

每个虚拟地址为 64 位，其中高 25 位无效，后 39 位被分为 9、9、9、12 四个部分，分别对应三级页表、二级页表、一级页表和页内偏移量。单个物理页大小为 4096(PGSIZE) 字节。

SV39 地址管理机制采用三级页表结构，页表项的大小为 8 字节，其中高 10 位无效，第 53 位到第 10 位承载下一级页表或数据页的物理页号，末尾 10 位用于标志位，记录有效位、读、写、执行及用户态访问权限。

![虚拟地址映射](images/页表机制.jpg)

内核空间的映射中，从 KERNBASE 至 PHYSTOP 之间的空间被直接映射，KERNBASE下面有几块地址用于设备管理，也是直接映射至虚拟内存。除此以外，该区域的顶部被映射至代码段中的 trampoline段(前文的布局中有提到，是用于用户态和内核态之间切换时的上下文保存的代码段)，用于快速切换至内核态。紧接着，该区域下方分配了 64 页的物理内存、128页的虚拟内存用于内核栈，并通过使用 guard page 在每两页栈之间进行保护，以及时捕获潜在的栈溢出事件。

用户空间的映射采用随机的物理页映射方式。在进程创建（如 fork 或 exec）时，操作系统会调用 kalloc 以获取物理页，并调用 uvmmap 函数进行代码段、数据段、栈段、trapframe 页及 trampoline 页的映射。此时，堆段的映射将在懒分配部分中进一步说明，共享内存段的映射将在后续的进程间通信讨论中展开。

2.1.4 伙伴系统
2.1.4.1 概述
伙伴系统和slab分配器是内核中用于分配物理内存的机制。其中，伙伴系统用于以页为单位分配物理页，slab分配器以更小的单位(例如8B，16B等等)进行分配小块的物理内存。

我们的伙伴系统仿照linux的思路，支持从0阶到MAX_ORDER = 11阶大小的内存块的分配，其中k阶大小的内存块的大小为2^k页，即2^(k + 12)B大小的连续内存。

我们将空闲页(从end, 上文提到的内核代码和数据段结束后的首地址， 到PHYSTOP之间的物理内存)以PGSIZE为单位，按照物理地址从低到高从0开始依次编号。

伙伴系统对外开放的接口如下: 
void            kinit(void); // 初始化伙伴系统
void*           kbuddy_alloc(uint32 order); // 获取一块order阶大小的内存块
void            kbuddy_free(void* ptr, uint32 order); // 释放一块order阶大小的内存块
void*           kalloc(void); // 即为 kbuddy_alloc(0) 获取一页内存
void            kfree(void* ptr); // 即为 kbuddy_free(ptr, 0)释放一页内存

2.1.4.2 伙伴系统相关的数据结构

struct {
    char bitmap_page[2 * PGSIZE];
    struct run freelist_page[MAX_ORDER + 1]; // 可以理解成一个地址数组，freelist_page[i]中存放的是i阶内存块的链表首地址
    char* start;
    struct spinlock lock;
    struct ref_stru freelist; // 内存计数
} buddy;

bitmap_page是伙伴系统的位图，里面存储了各个以i号页为起始页的各阶内存块的空闲情况，布局如下:
![buddy的位视图](images/buddy的位视图.jpg)

其中某一位为0表示以i页号为起始页的k阶内存块为空闲的，为1则表示被占用着。我们用buddy_bitmap_offset[]数组来记录各阶内存块的位视图在bitmap_page中的偏移量，其中buddy_bitmap_offset[k]中记录了各个k阶空闲块的位视图的偏移量。

这种多阶的位视图对比一阶的位视图，仅仅多用了一页的位视图，就能让判断以任意页号为起始页的任意阶内存块和它的伙伴块是否为空闲的复杂度，从原本o(2^k)变为了o(1)的时间。

freelist_page是一个地址数组，freelist_page[i]中存放的是i阶内存块的链表首地址。其中run的结构如下：

struct run {
    struct run* prev;
    struct run* next;
};

这是一个双向链表的结点，用于原地地存储某个空闲块的上一块内存和下一块内存的地址。这个链表的布局如下：

我们将空闲块的链表信息直接存储在空闲块的内部，这种隐式链表的优化减少了额外的内存开销和额外的指针存储，降低了内存访问次数，系统可以更快地找到和管理内存块，提高了内存分配的效率。同时，内存块地址即链表结点地址的设计也让我们能够以o(1)的时间内对链表中部的结点进行"随机访问"，兼具了线性结构和链式结构的优点。

start为0号内存的首地址，用于页号和物理地址的转换。

lock是一个内核自旋锁，用于保护buddy内部成员的并发访问。

freelist(TODO)

2.1.4.3 伙伴系统的初始化

在xxOS的初始化中，0号cpu会调用buddy_init()来进行伙伴系统的初始化。

void buddy_init(){
    initlock(&buddy.lock, "buddy"); // 初始化自旋锁

    pa_start = (char*)PGROUNDUP((uint64)end); // 将kernel.ld提供的end地址按照PGSIZE对齐
    buddy.start = pa_start; // 设置buddy管理的内存段的起始地址为start
    pnum_max = (PHYSTOP - (uint64)buddy.start) / PGSIZE; // 最大页号
    printf("pnum_max: %lu\n", pnum_max);

    buddy_init_bitmap(); // 将位视图置为全1，即全部被占用。
    buddy_init_freelist(); // 根据pnum_max，将从pa_start到PYHSTOP之间的各块物理页按照最大适配的算法加入到buddy.freelist_page中，并且将位视图的对应位置0，即置为空闲
}

2.1.4.4 伙伴系统的分配

函数 void* buddy_alloc(uint32 order) 的任务是分配一块
2^order页大小的连续物理内存。其算法流程如下：

从 buddy.freelist_page[order] 开始查找第一个空闲块，采用链表形式的存储结构，在O(1)时间复杂度内直接找到所需阶数的空闲块。
核心逻辑为当找到的空闲块超过目标阶数时，系统将其分步拆分为所需大小，同时将上半部分返回至对应阶数的空闲链表。
以下是分配的具体实现：
void* buddy_alloc(uint32 order)
{
    if (order > MAX_ORDER)
        panic("buddy_alloc: order out of bound.");
    int free_order = order;
    // 找到第一个空闲块
    while(free_order <= MAX_ORDER && (buddy_getfirst(free_order) == 0))
    {
        ++free_order;
    }

    if (free_order > MAX_ORDER)
        return NULL;
        // panic("buddy_alloc: bad alloc.");
    
    // 将free_order给逐步拆分成order阶的内存块
    void* mem_block = buddy_erasefirst(free_order);
    if (free_order > order)
    {
        while(free_order > order)
        {
            --free_order;
            void* upper_block = (void*)((uint64)mem_block + (1 << free_order) * PGSIZE);
            buddy_insertfirst(upper_block, free_order); // 把地址相对较高的给存入链表中
        }
    }
    return mem_block;
}

2.1.4.5 伙伴系统的释放

函数 void buddy_free(void* block_start, uint32 order) 的任务是释放一块 2^order 页大小的物理内存。其算法流程如下：

首先，函数检查传入的内存块起始地址 block_start 是否符合对齐要求。
接着，计算该内存块的伙伴块地址，并检查它是否空闲。
如果伙伴块为空闲状态，则从相应的空闲链表中将其移除，并向高阶合并；否则，将当前块加入到空闲链表中。
这个过程将一直继续，直到无法合并为止，或者达到了最大阶数。

由于我们的位视图可以在o(1)的时间内检查一个任意阶的内存块的伙伴是否为空闲的，所以这整个过程的复杂度也为o(1)。

以下是释放的具体实现：

void buddy_free(void* block_start, uint32 order)
{
    // 将物理地址转换为页号
    uint64 pnum = pa_to_pnum(block_start);
    
    // 检查页号是否符合对齐要求
    if (pnum & ((1 << order) - 1))
    {
        // 数据不符合要求，输出错误并终止程序
        printf("%lu\n", pnum);
        panic("buddy_free: unaligned block.");
    }

    // 初始化自由阶为要释放的阶
    uint32 freeorder = order;
    
    // 尝试合并直到达到最大阶数
    while(freeorder < MAX_ORDER)
    {
        // 计算伙伴的页号（按位异或）
        uint64 buddy_pnum = pnum ^ (1 << freeorder);
        
        // 检查伙伴块是否在有效范围内且为空闲
        if (buddy_pnum < pnum_max && !buddy_getbitmap(buddy_pnum, freeorder))
        {
            // 伙伴块空闲，进行合并
            // 从 corresponding 链表中移除伙伴块
            struct run* buddy_ptr = pnum_to_pa(buddy_pnum); 
            buddy_erase(buddy_ptr, freeorder); // 移除伙伴

            // 更新当前块的页号（按位清除对应位）
            pnum &= ~(1 << freeorder);
            // 提高自由阶级
            ++freeorder;
        }
        else
        {
            // 伙伴块不空闲，将当前块放入 freeorder 的链表中
            buddy_insertfirst(pnum_to_pa(pnum), freeorder);
            break; // 不再合并，直接退出
        }
    }

    // 如果合并到达最大阶数，则将当前块加入到相应链表
    if (freeorder == MAX_ORDER)
    {
        buddy_insertfirst(pnum_to_pa(pnum), freeorder);
    }

    return; // 释放过程结束
}

2.1.5 slab分配器

2.1.5.1 概述
Slab 分配器是内核中用于高效分配小块物理内存的机制。与伙伴系统以页为单位分配物理内存不同，Slab 分配器以更小的内存块（例如 8B、16B 等）进行分配，这样可以减少内存碎片并提高内存利用率。

Slab 分配器使用一个名为 kmem_cache 的结构体来管理不同大小的内存块池，支持小对象（小于等于 16B）和大对象（大于 16B）的分配。它通过将空闲块组织为链表的方式，提高了内存分配和释放的效率。

在实现的过程中，我们对小对象和大对象的slab池的结构作了不同的优化，更大限度地利用了空闲内存。



2.1.6 懒分配

TODO

2.1.7 写时复制

TODO

2.2 进程通信
2.2.1 概述
2.2.2 进程控制块
2.2.3 IPC宏
2.2.3 共享内存
2.2.4 信号量
2.2.5 消息队列
2.3 进程管理
2.3.1 概述
2.3.2 优先级队列调度
2.3.3 vfork
2.4 辅助性的系统调用
2.4.1 时间相关
2.4.2 系统调用跟踪
三、总结与展望
3.1 工作总结
3.2 经验总结
3.3 未来计划